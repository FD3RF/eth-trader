# -*- coding: utf-8 -*-
"""
ğŸš€ ç»ˆæé‡åŒ–ç»ˆç«¯ Â· å‡†æœºæ„ç‰ˆ 50.0
===================================================
æ ¸å¿ƒç‰¹æ€§ï¼ˆ4.5æ˜Ÿ å‡†æœºæ„çº§ï¼‰ï¼š
- ç‹¬ç«‹é£é™©å¼•æ“ï¼ˆåŠ¨æ€ä»“ä½ç¼©æ”¾ã€ç†”æ–­ã€åæ–¹å·®VaRã€å‹åŠ›æµ‹è¯•ã€æç«¯é£é™©æ£€æµ‹ï¼‰
- æ¦‚ç‡æ ¡å‡†ç³»ç»Ÿï¼ˆPlatt/Isotonicæ ¡å‡†ã€Brier Scoreã€ç½®ä¿¡åŒºé—´ã€æ»šåŠ¨å‘½ä¸­ç‡ï¼‰
- ç ”ç©¶æ¡†æ¶ï¼ˆWalk-ForwardéªŒè¯ã€æ ·æœ¬å¤–æŠ¥å‘Šã€å› å­ICè¡°å‡æ›²çº¿ã€Monte Carloæ¨¡æ‹Ÿï¼‰
- æ‰§è¡Œå±‚ä¼˜åŒ–ï¼ˆæ»‘ç‚¹æ¨¡å‹ã€æµåŠ¨æ€§è¿‡æ»¤ã€å»¶è¿Ÿæ•æ„Ÿæµ‹è¯•ï¼‰
- ç³»ç»Ÿè‡ªæ£€æ¨¡å—ï¼ˆæ¯æ—¥æ•°æ®/å› å­/IC/é£é™©å¼‚å¸¸æ£€æµ‹ï¼Œè‡ªåŠ¨é™çº§ï¼‰
- ä¿ç•™åŸæœ‰æ‰€æœ‰é«˜çº§åŠŸèƒ½ï¼ˆé£é™©é¢„ç®—ã€æ³¢åŠ¨ç‡ä»“ä½ã€æœŸæœ›æ”¶ç›Šæ’åºã€æœºå™¨å­¦ä¹ å› å­ç­‰ï¼‰
===================================================
"""

import streamlit as st
import pandas as pd
import numpy as np
import ta
import ccxt
import requests
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime, timedelta
from streamlit_autorefresh import st_autorefresh
import warnings
import time
import logging
import sys
import traceback
from typing import Optional, Dict, List, Tuple, Any, Union
from dataclasses import dataclass, field
from enum import Enum
from collections import deque, defaultdict
import functools
import hashlib
import csv
import os
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor, as_completed
from scipy.stats import ttest_1samp, norm, genpareto, ttest_ind
import pytz
import joblib
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay
from sklearn.isotonic import IsotonicRegression
from sklearn.linear_model import LogisticRegression
from hmmlearn import hmm
import pickle
import gym
from gym import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
import torch
import seaborn as sns
import matplotlib.pyplot as plt

warnings.filterwarnings('ignore')

# ==================== å…¨å±€å¼‚å¸¸æ•è· ====================
def global_exception_handler(exc_type, exc_value, exc_traceback):
    if issubclass(exc_type, KeyboardInterrupt):
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return
    logging.error("Uncaught exception", exc_info=(exc_type, exc_value, exc_traceback))
    with open('crash.log', 'a') as f:
        f.write(f"\n--- {datetime.now()} ---\n")
        traceback.print_exception(exc_type, exc_value, exc_traceback, file=f)

sys.excepthook = global_exception_handler

# ==================== æ—¥å¿—æ–‡ä»¶æŒä¹…åŒ– ====================
LOG_DIR = "logs"
TRADE_LOG_FILE = "trade_log.csv"
PERF_LOG_FILE = "performance_log.csv"
SLIPPAGE_LOG_FILE = "slippage_log.csv"
EQUITY_CURVE_FILE = "equity_curve.csv"
REGIME_STATS_FILE = "regime_stats.csv"
CONSISTENCY_FILE = "consistency_stats.csv"
MODEL_DIR = "models"
os.makedirs(LOG_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)

def append_to_csv(file_path: str, row: dict):
    file_exists = os.path.isfile(file_path)
    try:
        with open(file_path, 'a', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=row.keys())
            if not file_exists:
                writer.writeheader()
            writer.writerow(row)
    except Exception as e:
        print(f"å†™å…¥CSVå¤±è´¥: {e}")

def load_csv(file_path: str) -> pd.DataFrame:
    if os.path.exists(file_path):
        return pd.read_csv(file_path)
    return pd.DataFrame()

def append_to_log(file_name: str, message: str):
    date_str = datetime.now().strftime("%Y-%m-%d")
    log_path = os.path.join(LOG_DIR, f"{file_name}_{date_str}.log")
    try:
        with open(log_path, 'a', encoding='utf-8') as f:
            f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {message}\n")
    except Exception as e:
        print(f"å†™å…¥æ—¥å¿—å¤±è´¥: {e}")

# ==================== é…ç½®ä¸å¸¸é‡ ====================
class SignalStrength(Enum):
    EXTREME = 0.85
    STRONG = 0.75
    HIGH = 0.65
    MEDIUM = 0.55
    WEAK = 0.50
    NONE = 0.0

class MarketRegime(Enum):
    TREND = "è¶‹åŠ¿"
    RANGE = "éœ‡è¡"
    PANIC = "ææ…Œ"
    CALM = "å¹³é™"
    HIGH_VOL = "é«˜æ³¢åŠ¨"
    LOW_VOL = "ä½æ³¢åŠ¨"

class VaRMethod(Enum):
    NORMAL = "æ­£æ€æ³•"
    HISTORICAL = "å†å²æ¨¡æ‹Ÿæ³•"
    EXTREME = "æå€¼æ³•"
    VOLCONE = "æ³¢åŠ¨ç‡é”¥æ³•"

@dataclass
class TradingConfig:
    """æ‰€æœ‰å¯è°ƒå‚æ•°é›†ä¸­ç®¡ç†ï¼ˆå‡†æœºæ„ç‰ˆæ‰©å±•ï¼‰"""
    symbols: List[str] = field(default_factory=lambda: ["ETH/USDT", "BTC/USDT", "SOL/USDT", "BNB/USDT"])
    # ========== é£é™©é¢„ç®—æ¨¡å‹ ==========
    risk_per_trade: float = 0.008          # åŸºç¡€å•ç¬”é£é™©æ¯”ä¾‹ï¼ˆè´¦æˆ·ä½™é¢çš„0.8%ï¼‰
    daily_risk_budget_ratio: float = 0.025 # æ¯æ—¥é£é™©é¢„ç®—æ¯”ä¾‹ï¼ˆ2.5%ï¼‰
    # å¼ºåŒ–å­¦ä¹ ä»“ä½ç®¡ç†
    use_rl_position: bool = False          # æ˜¯å¦å¯ç”¨RLä»“ä½è°ƒæ•´ï¼ˆéœ€è®­ç»ƒæ¨¡å‹ï¼‰
    rl_model_path: str = "models/rl_ppo.zip"  # RLæ¨¡å‹ä¿å­˜è·¯å¾„
    rl_action_low: float = 0.5              # RLè¾“å‡ºçš„æœ€ä½é£é™©ä¹˜æ•°
    rl_action_high: float = 2.0             # RLè¾“å‡ºçš„æœ€é«˜é£é™©ä¹˜æ•°
    # å†·å´ä¸ç†”æ–­
    max_consecutive_losses: int = 3
    cooldown_losses: int = 3
    cooldown_hours: int = 24
    max_drawdown_pct: float = 20.0
    circuit_breaker_atr: float = 5.0
    circuit_breaker_fg_extreme: Tuple[int, int] = (10, 90)
    # æ³¢åŠ¨ç‡ç›¸å…³
    atr_multiplier_base: float = 1.5
    atr_multiplier_min: float = 1.2
    atr_multiplier_max: float = 2.5
    tp_min_ratio: float = 2.0               # æ­¢ç›ˆ/æ­¢æŸæœ€å°æ¯”ä¾‹
    partial_tp_ratio: float = 0.5
    partial_tp_r_multiple: float = 1.2
    breakeven_trigger_pct: float = 1.5
    max_hold_hours: int = 36
    min_atr_pct: float = 0.5                # æœ€å°ATRç™¾åˆ†æ¯”ï¼ˆç”¨äºè®¡ç®—æ­¢æŸè·ç¦»ä¸‹é™ï¼‰
    # å‡¯åˆ©ç›¸å…³ï¼ˆä¿ç•™ä½†æœªä½¿ç”¨ï¼‰
    kelly_fraction: float = 0.25
    # äº¤æ˜“æ‰€ä¸æ•°æ®
    exchanges: Dict[str, Any] = field(default_factory=lambda: {
        "Binanceåˆçº¦": ccxt.binance,
        "Bybitåˆçº¦": ccxt.bybit,
        "OKXåˆçº¦": ccxt.okx
    })
    data_sources: List[str] = field(default_factory=lambda: ["binance", "bybit", "okx", "mexc", "kucoin"])
    timeframes: List[str] = field(default_factory=lambda: ['1m', '5m', '15m', '1h', '4h', '1d'])
    confirm_timeframes: List[str] = field(default_factory=lambda: ['5m', '15m', '1h'])
    timeframe_weights: Dict[str, int] = field(default_factory=lambda: {'1d': 10, '4h': 7, '1h': 5, '15m': 3, '5m': 2, '1m': 1})
    fetch_limit: int = 2000
    auto_refresh_ms: int = 30000
    anti_duplicate_seconds: int = 180
    # æ»‘ç‚¹ä¸æ‰‹ç»­è´¹
    slippage_base: float = 0.0005
    slippage_impact_factor: float = 0.1
    slippage_imbalance_factor: float = 0.5
    fee_rate: float = 0.0004
    # å› å­ä¸æœºå™¨å­¦ä¹ 
    use_ml_factor: bool = True
    ml_retrain_interval: int = 3600
    ml_window: int = 500
    ml_n_estimators: int = 50
    ml_max_depth: int = 5
    use_prob_calibration: bool = True
    calibration_method: str = "isotonic"
    bayesian_prior_strength: float = 1.0
    factor_corr_threshold: float = 0.7
    factor_corr_penalty: float = 0.7
    ic_decay_rate: float = 0.99
    factor_eliminate_pvalue: float = 0.1
    factor_eliminate_ic: float = 0.02
    factor_min_weight: float = 0.1
    ic_window: int = 80
    factor_learning_rate: float = 0.3
    # åæ–¹å·®é£é™©é¢„ç®—
    risk_budget_method: str = "risk_parity"
    black_litterman_tau: float = 0.05
    cov_matrix_window: int = 50
    max_sector_exposure: float = 0.3
    # è®¢å•æ‹†åˆ†
    max_order_split: int = 3
    min_order_size: float = 0.001
    split_delay_seconds: int = 5
    # å¸‚åœºçŠ¶æ€è¿‡æ»¤
    regime_allow_trade: List[MarketRegime] = field(default_factory=lambda: [MarketRegime.TREND, MarketRegime.PANIC])
    # å¤œé—´æ¨¡å¼
    night_start_hour: int = 0
    night_end_hour: int = 8
    night_risk_multiplier: float = 0.5
    # æ³¢åŠ¨ç‡é”¥
    volcone_percentiles: List[float] = field(default_factory=lambda: [0.01, 0.05, 0.5, 0.95, 0.99])
    volcone_window: int = 100
    # å…¶ä»–
    var_confidence: float = 0.95
    var_method: VaRMethod = VaRMethod.HISTORICAL
    portfolio_risk_target: float = 0.02
    var_aggressive_threshold: float = 1.0
    adapt_window: int = 20
    atr_price_history_len: int = 20
    funding_rate_threshold: float = 0.05
    max_leverage_global: float = 10.0        # å…¨å±€æœ€å¤§æ æ†ï¼ˆå®‰å…¨ç½‘ï¼‰
    # å¼‚å¸¸æ£€æµ‹é˜ˆå€¼
    max_reasonable_balance: float = 1e7
    max_reasonable_daily_pnl_ratio: float = 10.0
    # HMM
    regime_detection_method: str = "hmm"
    hmm_n_components: int = 3
    hmm_n_iter: int = 100
    # æˆæœ¬æ„ŸçŸ¥è®­ç»ƒ
    cost_aware_training: bool = True
    # æ¨¡æ‹Ÿæ•°æ®å‚æ•°
    sim_volatility: float = 0.06
    sim_trend_strength: float = 0.2
    # å¸ƒæ—å¸¦å®½åº¦é˜ˆå€¼ï¼ˆç”¨äºéœ‡è¡åˆ¤æ–­ï¼‰
    bb_width_threshold: float = 0.1
    bb_window: int = 20
    rsi_range_low: int = 40
    rsi_range_high: int = 60
    # ========== é«˜çº§æ‰©å±•å‚æ•° ==========
    # å¤šæ•°æ®æº
    use_chain_data: bool = False             # æ˜¯å¦ä½¿ç”¨é“¾ä¸Šæ•°æ®
    chain_api_key: str = ""                  # CryptoQuant API Key
    use_orderflow: bool = False              # æ˜¯å¦ä½¿ç”¨è®¢å•æµæ•°æ®
    use_sentiment: bool = True                # æ˜¯å¦ä½¿ç”¨èˆ†æƒ…æ•°æ®ï¼ˆææƒ§è´ªå©ªæŒ‡æ•°ï¼‰
    # è‡ªé€‚åº”ä¼˜åŒ–
    auto_optimize_interval: int = 86400      # è‡ªé€‚åº”ä¼˜åŒ–é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤24å°æ—¶
    optimize_window: int = 30                 # ä¼˜åŒ–çª—å£ï¼ˆå¤©æ•°ï¼‰
    # äº¤æ˜“æˆæœ¬å»ºæ¨¡å¢å¼º
    cost_model_version: str = "v2"            # æˆæœ¬æ¨¡å‹ç‰ˆæœ¬ï¼ˆv2ä¸ºå†²å‡»+æ»‘ç‚¹ï¼‰
    # ========== 4.5æ˜Ÿæ–°å¢å‚æ•° ==========
    # é£é™©å¼•æ“å‚æ•°
    drawdown_scaling_factor: float = 2.0      # å›æ’¤è¶…è¿‡5%åçš„ä»“ä½ç¼©å‡å› å­
    var_scaling_factor: float = 1.5           # VaRè¶…è¿‡ç›®æ ‡åçš„ä»“ä½ç¼©å‡å› å­
    daily_loss_limit: float = 0.025           # æ—¥äºæŸä¸Šé™ï¼ˆä¸é£é™©é¢„ç®—ä¸€è‡´ï¼‰
    cooldown_hours_risk: int = 12             # é£é™©è§¦å‘å†·å´æ—¶é•¿
    # å‹åŠ›æµ‹è¯•åœºæ™¯
    stress_scenarios: Dict[str, Dict[str, float]] = field(default_factory=lambda: {
        'btc_crash_10': {'BTC/USDT': -0.10, 'ETH/USDT': -0.15},
        'vol_double': {'volatility_multiplier': 2.0},
        'liquidity_half': {'volume_multiplier': 0.5}
    })
    # æ¦‚ç‡æ ¡å‡†
    calibration_window: int = 200              # æ ¡å‡†çª—å£å¤§å°
    brier_score_window: int = 30               # Brier Scoreæ»šåŠ¨çª—å£
    # Walk-Forward
    walk_forward_train_pct: float = 0.7        # è®­ç»ƒé›†æ¯”ä¾‹
    walk_forward_step: int = 100               # æ»šåŠ¨æ­¥é•¿
    # è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ
    mc_simulations: int = 1000                  # æ¨¡æ‹Ÿæ¬¡æ•°
    # æ‰§è¡Œå±‚ä¼˜åŒ–
    max_order_to_depth_ratio: float = 0.01      # è®¢å•å¤§å°/ç›˜å£æ·±åº¦æœ€å¤§æ¯”ä¾‹
    latency_sim_ms: int = 200                   # æ¨¡æ‹Ÿå»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
    # è‡ªæ£€
    self_check_interval: int = 3600             # è‡ªæ£€é—´éš”ï¼ˆç§’ï¼‰
    anomaly_threshold_ic_drop: float = 0.5      # ICä¸‹é™é˜ˆå€¼ï¼ˆå€æ•°ï¼‰

CONFIG = TradingConfig()

# ==================== å…¨å±€å˜é‡ ====================
factor_weights = {
    'trend': 1.0,
    'rsi': 1.0,
    'macd': 1.0,
    'bb': 1.0,
    'volume': 1.0,
    'adx': 1.0,
    'ml': 1.0
}
factor_to_col = {
    'trend': 'trend_factor',
    'rsi': 'rsi',
    'macd': 'macd_diff',
    'bb': 'bb_factor',
    'volume': 'volume_ratio',
    'adx': 'adx',
    'ml': 'ml_factor'
}

ic_decay_records = {f: deque(maxlen=200) for f in factor_weights}
factor_corr_matrix = None

ml_models = {}
ml_scalers = {}
ml_feature_cols = {}
ml_last_train = {}
ml_calibrators = {}

volcone_cache = {}
hmm_models = {}
hmm_last_train = {}

# å¼ºåŒ–å­¦ä¹ ç›¸å…³
rl_model = None
rl_env = None

# è‡ªé€‚åº”ä¼˜åŒ–ç›¸å…³
last_optimize_time = 0

# è‡ªæ£€ç›¸å…³
last_self_check_time = 0
self_check_status = {"healthy": True, "messages": []}

# ==================== æ—¥å¿—ç³»ç»Ÿ ====================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("UltimateTrader")

def safe_request(max_retries: int = 3, default=None):
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {e}")
                    if attempt == max_retries - 1:
                        return default
                    time.sleep(2 ** attempt)
            return default
        return wrapper
    return decorator

def init_session_state():
    """åˆå§‹åŒ–Streamlitä¼šè¯çŠ¶æ€ï¼Œä»CSVæ¢å¤æŒä¹…æ•°æ®"""
    equity_df = load_csv(EQUITY_CURVE_FILE)
    equity_curve = deque(maxlen=500)
    if not equity_df.empty:
        for _, row in equity_df.iterrows():
            try:
                t = pd.to_datetime(row['time'])
                equity_curve.append({'time': t, 'equity': float(row['equity'])})
            except:
                pass

    regime_stats = {}
    regime_df = load_csv(REGIME_STATS_FILE)
    if not regime_df.empty:
        for _, row in regime_df.iterrows():
            regime_stats[row['regime']] = {
                'trades': int(row['trades']),
                'wins': int(row['wins']),
                'total_pnl': float(row['total_pnl'])
            }

    consistency_stats = {'backtest': {}, 'live': {}}
    cons_df = load_csv(CONSISTENCY_FILE)
    if not cons_df.empty:
        for _, row in cons_df.iterrows():
            typ = row['type']
            consistency_stats[typ] = {
                'trades': int(row['trades']),
                'avg_slippage': float(row['avg_slippage']),
                'win_rate': float(row['win_rate'])
            }

    defaults = {
        'account_balance': 10000.0,
        'daily_pnl': 0.0,
        'daily_risk_consumed': 0.0,
        'peak_balance': 10000.0,
        'consecutive_losses': 0,
        'daily_trades': 0,
        'trade_log': [],
        'positions': {},
        'auto_enabled': True,
        'pause_until': None,
        'exchange': None,
        'net_value_history': [],
        'equity_curve': equity_curve,
        'last_signal_time': {},
        'current_symbols': ['ETH/USDT', 'BTC/USDT'],
        'telegram_token': None,
        'telegram_chat_id': None,
        'backtest_results': None,
        'wf_results': None,
        'param_sensitivity': None,
        'circuit_breaker': False,
        'cooldown_until': None,
        'mc_results': None,
        'use_simulated_data': False,
        'data_source_failed': False,
        'error_log': deque(maxlen=20),
        'execution_log': deque(maxlen=50),
        'last_trade_date': None,
        'exchange_choice': 'Binanceåˆçº¦',
        'testnet': True,
        'use_real': False,
        'binance_api_key': '',
        'binance_secret_key': '',
        'fear_greed': 50,
        'market_regime': MarketRegime.RANGE,
        'multi_df': {},
        'performance_metrics': {},
        'mode': 'live',
        'factor_ic_stats': {},
        'symbol_current_prices': {},
        'daily_returns': deque(maxlen=252),
        'cov_matrix': None,
        'cov_matrix_cache': {'key': None, 'matrix': None},
        'slippage_records': [],
        'regime_stats': regime_stats,
        'consistency_stats': consistency_stats,
        'aggressive_mode': False,
        'dynamic_max_daily_trades': 9999,
        'var_method': CONFIG.var_method.value,
        'funding_rates': {},
        'ml_factor_scores': {},
        'volcone': None,
        'adaptive_params': {},
        'sector_exposure': {},
        'hmm_regime': None,
        'calibration_model': None,
        'walk_forward_index': 0,
        # é«˜çº§æ‰©å±•çŠ¶æ€
        'factor_exposure': {},
        'slippage_history': deque(maxlen=100),
        'chain_data': {},
        'orderflow_data': {},
        'optimization_history': [],
        # 4.5æ˜Ÿæ–°å¢çŠ¶æ€
        'brier_scores': deque(maxlen=100),
        'ic_history': {f: deque(maxlen=100) for f in factor_weights.keys()},
        'stress_test_results': None,
        'mc_results': None,
        'walk_forward_report': None,
        'self_check_status': self_check_status,
    }
    for k, v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v

def check_and_fix_anomalies():
    """æ£€æŸ¥å¹¶ä¿®å¤å¼‚å¸¸æ•°æ®ï¼ˆä½™é¢ã€daily_pnlã€æŒä»“ç­‰ï¼‰"""
    fixed = False
    if st.session_state.account_balance > CONFIG.max_reasonable_balance or st.session_state.account_balance < 0:
        log_error(f"æ£€æµ‹åˆ°å¼‚å¸¸ä½™é¢ {st.session_state.account_balance:.2f}ï¼Œè‡ªåŠ¨é‡ç½®ä¸º10000")
        st.session_state.account_balance = 10000.0
        st.session_state.peak_balance = 10000.0
        fixed = True
    if abs(st.session_state.daily_pnl) > st.session_state.account_balance * CONFIG.max_reasonable_daily_pnl_ratio:
        log_error(f"æ£€æµ‹åˆ°å¼‚å¸¸æ¯æ—¥ç›ˆäº {st.session_state.daily_pnl:.2f}ï¼Œè‡ªåŠ¨é‡ç½®ä¸º0")
        st.session_state.daily_pnl = 0.0
        fixed = True
    if abs(st.session_state.daily_risk_consumed) > st.session_state.account_balance * 0.5:
        log_error(f"æ£€æµ‹åˆ°å¼‚å¸¸é£é™©æ¶ˆè€— {st.session_state.daily_risk_consumed:.2f}ï¼Œè‡ªåŠ¨é‡ç½®ä¸º0")
        st.session_state.daily_risk_consumed = 0.0
        fixed = True
    if fixed:
        for f in [EQUITY_CURVE_FILE, TRADE_LOG_FILE, REGIME_STATS_FILE, CONSISTENCY_FILE]:
            if os.path.exists(f):
                os.remove(f)
        st.session_state.trade_log = []
        st.session_state.equity_curve.clear()
        st.rerun()

def log_error(msg: str):
    st.session_state.error_log.append(f"{datetime.now().strftime('%H:%M:%S')} - {msg}")
    append_to_log("error", msg)
    logger.error(msg)

def log_execution(msg: str):
    st.session_state.execution_log.append(f"{datetime.now().strftime('%H:%M:%S')} - {msg}")
    append_to_log("execution", msg)

def send_telegram(msg: str, msg_type: str = "info", image: Optional[Any] = None):
    token = st.session_state.get('telegram_token')
    chat_id = st.session_state.get('telegram_chat_id')
    if not token or not chat_id:
        return
    try:
        if image is not None:
            import io
            buf = io.BytesIO()
            image.write_image(buf, format='png')
            buf.seek(0)
            files = {'photo': buf}
            requests.post(f"https://api.telegram.org/bot{token}/sendPhoto",
                          data={'chat_id': chat_id}, files=files, timeout=5)
        else:
            prefix = {
                'info': 'â„¹ï¸ ',
                'signal': 'ğŸ“Š ',
                'risk': 'âš ï¸ ',
                'trade': 'ğŸ”„ '
            }.get(msg_type, '')
            full_msg = f"{prefix}{msg}"
            requests.post(f"https://api.telegram.org/bot{token}/sendMessage",
                          json={"chat_id": chat_id, "text": full_msg}, timeout=3)
    except Exception as e:
        logger.warning(f"Telegramå‘é€å¤±è´¥: {e}")

def update_performance_metrics():
    trades = st.session_state.trade_log[-100:]
    if len(trades) < 10:
        return
    df = pd.DataFrame(trades)
    wins = df[df['pnl'] > 0]
    losses = df[df['pnl'] < 0]
    win_rate = len(wins) / len(df) if len(df) > 0 else 0
    avg_win = wins['pnl'].mean() if not wins.empty else 0
    avg_loss = abs(losses['pnl'].mean()) if not losses.empty else 1
    returns = df['pnl'].values / st.session_state.account_balance
    sharpe = (returns.mean() / returns.std() * np.sqrt(252)) if len(returns) > 1 and returns.std() != 0 else 0
    st.session_state.performance_metrics = {
        'win_rate': win_rate,
        'avg_win': avg_win,
        'avg_loss': avg_loss,
        'sharpe': sharpe
    }

def current_equity():
    balance = st.session_state.account_balance
    floating = 0.0
    for sym, pos in st.session_state.positions.items():
        if sym in st.session_state.symbol_current_prices:
            floating += pos.pnl(st.session_state.symbol_current_prices[sym])
    return balance + floating

def calculate_drawdown():
    if len(st.session_state.equity_curve) < 2:
        return 0.0, 0.0
    df = pd.DataFrame(list(st.session_state.equity_curve))
    peak = df['equity'].cummax()
    dd = (peak - df['equity']) / peak * 100
    current_dd = dd.iloc[-1]
    max_dd = dd.max()
    return current_dd, max_dd

def record_equity_point():
    equity = current_equity()
    now = datetime.now()
    st.session_state.equity_curve.append({'time': now, 'equity': equity})
    append_to_csv(EQUITY_CURVE_FILE, {'time': now.isoformat(), 'equity': equity})

def update_regime_stats(regime: MarketRegime, pnl: float):
    key = regime.value
    if key not in st.session_state.regime_stats:
        st.session_state.regime_stats[key] = {'trades': 0, 'wins': 0, 'total_pnl': 0.0}
    st.session_state.regime_stats[key]['trades'] += 1
    if pnl > 0:
        st.session_state.regime_stats[key]['wins'] += 1
    st.session_state.regime_stats[key]['total_pnl'] += pnl
    rows = []
    for k, v in st.session_state.regime_stats.items():
        rows.append({'regime': k, 'trades': v['trades'], 'wins': v['wins'], 'total_pnl': v['total_pnl']})
    pd.DataFrame(rows).to_csv(REGIME_STATS_FILE, index=False)

def update_consistency_stats(is_backtest: bool, slippage: float, win: bool):
    key = 'backtest' if is_backtest else 'live'
    stats = st.session_state.consistency_stats.get(key, {})
    stats['trades'] = stats.get('trades', 0) + 1
    stats['avg_slippage'] = (stats.get('avg_slippage', 0.0) * (stats['trades'] - 1) + slippage) / stats['trades']
    if win:
        stats['wins'] = stats.get('wins', 0) + 1
    stats['win_rate'] = stats.get('wins', 0) / stats['trades'] if stats['trades'] > 0 else 0
    st.session_state.consistency_stats[key] = stats
    rows = []
    for typ, s in st.session_state.consistency_stats.items():
        rows.append({
            'type': typ,
            'trades': s.get('trades', 0),
            'avg_slippage': s.get('avg_slippage', 0.0),
            'win_rate': s.get('win_rate', 0.0)
        })
    pd.DataFrame(rows).to_csv(CONSISTENCY_FILE, index=False)

# ==================== é£é™©é¢„ç®—æ£€æŸ¥ ====================
def check_and_reset_daily():
    """æ£€æŸ¥æ—¥æœŸå˜æ›´ï¼Œé‡ç½®æ¯æ—¥äº¤æ˜“æ¬¡æ•°ã€ç›ˆäºå’Œé£é™©æ¶ˆè€—"""
    today = datetime.now().date()
    if st.session_state.get('last_trade_date') != today:
        st.session_state.daily_trades = 0
        st.session_state.daily_pnl = 0.0
        st.session_state.daily_risk_consumed = 0.0
        st.session_state.last_trade_date = today
        log_execution("æ–°çš„ä¸€å¤©ï¼Œé‡ç½®æ¯æ—¥æ•°æ®")

def check_risk_budget() -> bool:
    """æ£€æŸ¥ä»Šæ—¥å·²æ¶ˆè€—é£é™©æ˜¯å¦è¶…è¿‡é¢„ç®—"""
    budget = st.session_state.account_balance * CONFIG.daily_risk_budget_ratio
    if st.session_state.daily_risk_consumed >= budget:
        return False
    return True

# ==================== è‡ªé€‚åº”ATRå€æ•°ï¼ˆåŸºäºæ³¢åŠ¨ç‡é”¥ï¼‰====================
def adaptive_atr_multiplier(price_series: pd.Series) -> float:
    if len(price_series) < CONFIG.adapt_window:
        return CONFIG.atr_multiplier_base
    returns = price_series.pct_change().dropna()
    vol = returns.std() * np.sqrt(365 * 24 * 4)
    volcone = get_volcone(returns)
    current_vol_percentile = np.mean(vol <= volcone['percentiles']) if volcone else 0.5
    factor = 1.5 - current_vol_percentile
    new_mult = CONFIG.atr_multiplier_base * factor
    return np.clip(new_mult, CONFIG.atr_multiplier_min, CONFIG.atr_multiplier_max)

def get_volcone(returns: pd.Series) -> dict:
    key = hash(tuple(returns[-CONFIG.volcone_window:]))
    if key in volcone_cache:
        return volcone_cache[key]
    windows = [5, 10, 20, 40, 60]
    volcone = {}
    vols = []
    for w in windows:
        roll_vol = returns.rolling(w).std() * np.sqrt(365*24*4/w)
        volcone[f'vol_{w}'] = roll_vol.dropna().quantile(CONFIG.volcone_percentiles).to_dict()
        vols.extend(roll_vol.dropna().values)
    volcone['percentiles'] = np.percentile(vols, [p*100 for p in CONFIG.volcone_percentiles])
    volcone_cache[key] = volcone
    return volcone

# ==================== Regimeæ£€æµ‹ï¼ˆHMMï¼‰====================
def train_hmm(symbol: str, df_dict: Dict[str, pd.DataFrame]) -> Optional[hmm.GaussianHMM]:
    df = df_dict['15m'].copy()
    ret = df['close'].pct_change().dropna().values.reshape(-1, 1)
    if len(ret) < 200:
        return None
    scaler = StandardScaler()
    ret_scaled = scaler.fit_transform(ret)
    model = hmm.GaussianHMM(n_components=CONFIG.hmm_n_components, covariance_type="diag", n_iter=CONFIG.hmm_n_iter)
    model.fit(ret_scaled)
    return model

def detect_hmm_regime(symbol: str, df_dict: Dict[str, pd.DataFrame]) -> int:
    now = time.time()
    if symbol not in hmm_models or now - hmm_last_train.get(symbol, 0) > CONFIG.ml_retrain_interval:
        model = train_hmm(symbol, df_dict)
        if model is not None:
            hmm_models[symbol] = model
            hmm_last_train[symbol] = now
    if symbol not in hmm_models:
        return 0
    model = hmm_models[symbol]
    df = df_dict['15m'].copy()
    ret = df['close'].pct_change().dropna().values[-50:].reshape(-1, 1)
    if len(ret) < 10:
        return 0
    scaler = StandardScaler()
    ret_scaled = scaler.fit_transform(ret)
    states = model.predict(ret_scaled)
    return states[-1]

def detect_market_regime_advanced(df_dict: Dict[str, pd.DataFrame], symbol: str) -> MarketRegime:
    if CONFIG.regime_detection_method == 'hmm':
        state = detect_hmm_regime(symbol, df_dict)
        mapping = {0: MarketRegime.RANGE, 1: MarketRegime.TREND, 2: MarketRegime.PANIC}
        return mapping.get(state, MarketRegime.RANGE)
    else:
        return detect_market_regime_traditional(df_dict)

def detect_market_regime_traditional(df_dict: Dict[str, pd.DataFrame]) -> MarketRegime:
    if '1h' not in df_dict or '4h' not in df_dict:
        return MarketRegime.RANGE
    df1h = df_dict['1h']
    df4h = df_dict['4h']
    adx1h = df1h['adx'].iloc[-1] if not pd.isna(df1h['adx'].iloc[-1]) else 25
    adx4h = df4h['adx'].iloc[-1] if not pd.isna(df4h['adx'].iloc[-1]) else 25
    avg_adx = (adx1h + adx4h) / 2
    close1h = df1h['close'].iloc[-1]
    ema20_1h = df1h['ema20'].iloc[-1] if not pd.isna(df1h['ema20'].iloc[-1]) else close1h
    close4h = df4h['close'].iloc[-1]
    ema20_4h = df4h['ema20'].iloc[-1] if not pd.isna(df4h['ema20'].iloc[-1]) else close4h
    trend_up = (close1h > ema20_1h) and (close4h > ema20_4h)
    trend_down = (close1h < ema20_1h) and (close4h < ema20_4h)
    if avg_adx > 30:
        if trend_up or trend_down:
            return MarketRegime.TREND
        else:
            return MarketRegime.RANGE
    elif st.session_state.fear_greed <= 20:
        return MarketRegime.PANIC
    else:
        return MarketRegime.RANGE

# ==================== å¤šæ•°æ®æºèåˆ ====================
@safe_request(max_retries=2, default=None)
def fetch_chain_data(symbol: str) -> Optional[Dict]:
    """è·å–é“¾ä¸Šæ•°æ®ï¼ˆç¤ºä¾‹ä½¿ç”¨CryptoQuant APIï¼‰"""
    if not CONFIG.use_chain_data or not CONFIG.chain_api_key:
        return None
    # è¿™é‡Œä»…ä½œç¤ºä¾‹ï¼Œå®é™…éœ€è¦æ ¹æ®APIæ–‡æ¡£è°ƒæ•´
    try:
        url = f"https://api.cryptoquant.com/v1/btc/exchange-flows/inflow?api_key={CONFIG.chain_api_key}"
        r = requests.get(url, timeout=5)
        data = r.json()
        return {
            'exchange_inflow': data.get('inflow', 0),
            'exchange_outflow': data.get('outflow', 0),
            'netflow': data.get('inflow', 0) - data.get('outflow', 0),
        }
    except:
        return None

@safe_request(max_retries=2, default=None)
def fetch_orderflow_data(symbol: str) -> Optional[Dict]:
    """è·å–è®¢å•æµæ•°æ®ï¼ˆç¤ºä¾‹ï¼Œå®é™…éœ€æ¥å…¥ä¸“ä¸šæ•°æ®æºï¼‰"""
    if not CONFIG.use_orderflow:
        return None
    # æ¨¡æ‹Ÿæ•°æ®
    return {
        'bid_ask_imbalance': np.random.uniform(-0.5, 0.5),
        'cumulative_delta': np.random.uniform(-1000, 1000),
    }

def fetch_sentiment() -> int:
    """è·å–èˆ†æƒ…æ•°æ®ï¼ˆææƒ§è´ªå©ªæŒ‡æ•°ï¼‰"""
    return fetch_fear_greed()

def update_alternative_data(symbol: str):
    """æ›´æ–°å¦ç±»æ•°æ®åˆ°session_state"""
    if CONFIG.use_chain_data:
        st.session_state.chain_data[symbol] = fetch_chain_data(symbol)
    if CONFIG.use_orderflow:
        st.session_state.orderflow_data[symbol] = fetch_orderflow_data(symbol)

# ==================== å¼ºåŒ–å­¦ä¹ ä»“ä½ç®¡ç† ====================
class TradingEnv(gym.Env):
    """å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼šæ ¹æ®å¸‚åœºçŠ¶æ€è¾“å‡ºé£é™©ä¹˜æ•°"""
    def __init__(self, data_dict):
        super(TradingEnv, self).__init__()
        self.data = data_dict  # åŒ…å«ä»·æ ¼ã€æŒ‡æ ‡ç­‰
        self.current_step = 0
        # åŠ¨ä½œç©ºé—´ï¼šé£é™©ä¹˜æ•° [0.5, 2.0]
        self.action_space = spaces.Box(low=CONFIG.rl_action_low, high=CONFIG.rl_action_high, shape=(1,), dtype=np.float32)
        # è§‚æµ‹ç©ºé—´ï¼šè¿‘æœŸæŒ‡æ ‡ï¼ˆå¦‚æ”¶ç›Šç‡ã€æ³¢åŠ¨ç‡ã€RSIç­‰ï¼‰
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(10,), dtype=np.float32)
        self.reset()

    def reset(self):
        self.current_step = 0
        return self._get_obs()

    def _get_obs(self):
        # æ„é€ è§‚æµ‹å‘é‡ï¼šæœ€è¿‘5æœŸæ”¶ç›Šç‡ã€ATRã€RSIã€å¸‚åœºçŠ¶æ€ç­‰
        obs = np.zeros(10)
        # ç®€åŒ–ç¤ºä¾‹
        return obs

    def step(self, action):
        # æ‰§è¡ŒåŠ¨ä½œï¼Œè®¡ç®—å¥–åŠ±ï¼ˆå¦‚æœªæ¥æ”¶ç›Šï¼‰
        reward = 0
        done = False
        info = {}
        self.current_step += 1
        return self._get_obs(), reward, done, info

def load_or_train_rl_model():
    """åŠ è½½æˆ–è®­ç»ƒå¼ºåŒ–å­¦ä¹ æ¨¡å‹"""
    global rl_model
    if os.path.exists(CONFIG.rl_model_path):
        rl_model = PPO.load(CONFIG.rl_model_path)
    else:
        # åˆ›å»ºç¯å¢ƒå¹¶è®­ç»ƒï¼ˆéœ€è¦å†å²æ•°æ®ï¼‰
        env = DummyVecEnv([lambda: TradingEnv(st.session_state.multi_df)])
        rl_model = PPO('MlpPolicy', env, verbose=0)
        rl_model.learn(total_timesteps=10000)
        rl_model.save(CONFIG.rl_model_path)
    return rl_model

def get_rl_risk_multiplier() -> float:
    """ä½¿ç”¨RLæ¨¡å‹è·å–å½“å‰é£é™©ä¹˜æ•°"""
    if not CONFIG.use_rl_position:
        return 1.0
    try:
        global rl_model
        if rl_model is None:
            rl_model = load_or_train_rl_model()
        # æ„å»ºå½“å‰è§‚æµ‹
        obs = np.zeros(10)  # éœ€è¦æ ¹æ®å®é™…æ•°æ®å¡«å……
        action, _ = rl_model.predict(obs, deterministic=True)
        return float(np.clip(action, CONFIG.rl_action_low, CONFIG.rl_action_high))
    except Exception as e:
        log_error(f"RLæ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
        return 1.0

# ==================== è‡ªé€‚åº”å‚æ•°ä¼˜åŒ– ====================
def optimize_parameters():
    """è‡ªé€‚åº”ä¼˜åŒ–å‚æ•°ï¼ˆå¦‚å› å­æƒé‡ã€æ­¢æŸå€æ•°ç­‰ï¼‰"""
    global last_optimize_time
    now = time.time()
    if now - last_optimize_time < CONFIG.auto_optimize_interval:
        return
    last_optimize_time = now
    log_execution("å¼€å§‹è‡ªé€‚åº”å‚æ•°ä¼˜åŒ–...")
    # è·å–å†å²äº¤æ˜“æ•°æ®
    df_trades = pd.DataFrame(st.session_state.trade_log)
    if len(df_trades) < 20:
        return
    # ç¤ºä¾‹ï¼šä¼˜åŒ–å› å­æƒé‡ï¼ˆä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–æˆ–ç½‘æ ¼æœç´¢ï¼‰
    # è¿™é‡Œç®€åŒ–ä¸ºæ›´æ–°å› å­æƒé‡ä¸ºå†å²ICçš„åŠ æƒå¹³å‡
    global factor_weights
    for factor in factor_weights.keys():
        if factor in st.session_state.factor_ic_stats:
            factor_weights[factor] = st.session_state.factor_ic_stats[factor]['mean']
    log_execution(f"å› å­æƒé‡æ›´æ–°ä¸º: {factor_weights}")

# ==================== æœºå™¨å­¦ä¹ å› å­ï¼ˆæˆæœ¬æ„ŸçŸ¥è®­ç»ƒv2ï¼‰====================
def train_ml_model_cost_aware_v2(symbol: str, df_dict: Dict[str, pd.DataFrame]) -> Tuple[Any, Any, List[str]]:
    """è®­ç»ƒéšæœºæ£®æ—é¢„æµ‹æœªæ¥å‡€æ”¶ç›Šï¼Œä½¿ç”¨v2æˆæœ¬æ¨¡å‹ï¼ˆå†²å‡»+æ»‘ç‚¹ï¼‰"""
    df = df_dict['15m'].copy()
    feature_cols = ['ema20', 'ema50', 'rsi', 'macd_diff', 'bb_width', 'volume_ratio', 'adx', 'atr']
    df = df.dropna(subset=feature_cols + ['close'])
    if len(df) < CONFIG.ml_window:
        return None, None, []
    # åˆ›å»ºæ»åç‰¹å¾
    for col in feature_cols:
        for lag in [1,2,3]:
            df[f'{col}_lag{lag}'] = df[col].shift(lag)
    # è®¡ç®—æœªæ¥æ”¶ç›Š
    future_ret = df['close'].pct_change(5).shift(-5)
    if CONFIG.cost_aware_training:
        # æ›´ç²¾ç»†çš„æˆæœ¬ä¼°è®¡ï¼šå†²å‡»æˆæœ¬ + æ»‘ç‚¹
        # å†²å‡»æˆæœ¬ä¼°è®¡ï¼ˆåŸºäºæˆäº¤é‡ï¼‰
        volume_ma = df['volume'].rolling(20).mean()
        impact = (df['volume'] / volume_ma).fillna(1) * 0.0002  # å†²å‡»å› å­
        # æ»‘ç‚¹ä¼°è®¡ï¼ˆåŸºäºæ³¢åŠ¨ç‡ï¼‰
        vola = df['atr'] / df['close']
        slippage_est = vola * 0.001
        total_cost = impact + slippage_est
        target = future_ret - total_cost.shift(-5)
    else:
        target = future_ret
    df['target'] = target
    df = df.dropna()
    if len(df) < 100:
        return None, None, []
    all_feature_cols = []
    for col in feature_cols:
        all_feature_cols.append(col)
        for lag in [1,2,3]:
            all_feature_cols.append(f'{col}_lag{lag}')
    X = df[all_feature_cols]
    y = df['target']
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    model = RandomForestRegressor(
        n_estimators=CONFIG.ml_n_estimators,
        max_depth=CONFIG.ml_max_depth,
        random_state=42
    )
    model.fit(X_scaled, y)
    return model, scaler, all_feature_cols

def get_ml_factor(symbol: str, df_dict: Dict[str, pd.DataFrame]) -> float:
    """è·å–æœºå™¨å­¦ä¹ å› å­å¾—åˆ†ï¼ˆå½’ä¸€åŒ–åˆ°[-1,1]ï¼‰ï¼Œä½¿ç”¨v2æ¨¡å‹"""
    if not CONFIG.use_ml_factor:
        return 0.0
    now = time.time()
    if symbol not in ml_models or now - ml_last_train.get(symbol, 0) > CONFIG.ml_retrain_interval:
        if CONFIG.cost_model_version == "v2":
            model, scaler, feature_cols = train_ml_model_cost_aware_v2(symbol, df_dict)
        else:
            model, scaler, feature_cols = train_ml_model_cost_aware_v2(symbol, df_dict)  # é»˜è®¤v2
        if model is not None:
            ml_models[symbol] = model
            ml_scalers[symbol] = scaler
            ml_feature_cols[symbol] = feature_cols
            ml_last_train[symbol] = now
    if symbol not in ml_models:
        return 0.0
    model = ml_models[symbol]
    scaler = ml_scalers[symbol]
    feature_cols = ml_feature_cols.get(symbol, [])
    if not feature_cols:
        return 0.0
    df = df_dict['15m'].copy()
    if len(df) < 4:
        return 0.0
    last_idx = -1
    data = {}
    feature_cols_original = ['ema20', 'ema50', 'rsi', 'macd_diff', 'bb_width', 'volume_ratio', 'adx', 'atr']
    for col in feature_cols_original:
        if col in df.columns:
            data[col] = df[col].iloc[last_idx]
        else:
            data[col] = np.nan
        for lag in [1,2,3]:
            lag_col = f'{col}_lag{lag}'
            if len(df) > lag:
                data[lag_col] = df[col].iloc[-lag-1]
            else:
                data[lag_col] = np.nan
    X = pd.DataFrame([data])
    X = X[feature_cols]
    X = X.fillna(0)
    X_scaled = scaler.transform(X)
    pred = model.predict(X_scaled)[0]
    return np.tanh(pred * 10)

# ==================== æ¦‚ç‡æ ¡å‡†ç³»ç»Ÿ ====================
def calibrate_probabilities(symbol: str, raw_probs: np.ndarray, true_labels: np.ndarray) -> Any:
    """è®­ç»ƒæ¦‚ç‡æ ¡å‡†å™¨ï¼Œè¿”å›æ ¡å‡†æ¨¡å‹"""
    if CONFIG.calibration_method == 'platt':
        from sklearn.calibration import CalibratedClassifierCV
        from sklearn.linear_model import LogisticRegression
        base_clf = LogisticRegression()
        calibrated = CalibratedClassifierCV(base_clf, method='sigmoid', cv='prefit')
        return None  # éœ€è¦å®é™…è®­ç»ƒ
    elif CONFIG.calibration_method == 'isotonic':
        from sklearn.isotonic import IsotonicRegression
        ir = IsotonicRegression(out_of_bounds='clip')
        ir.fit(raw_probs, true_labels)
        return ir
    return None

def apply_calibration(symbol: str, raw_prob: float) -> float:
    """åº”ç”¨æ ¡å‡†åçš„æ¦‚ç‡"""
    if not CONFIG.use_prob_calibration or symbol not in ml_calibrators:
        return raw_prob
    calibrator = ml_calibrators[symbol]
    if hasattr(calibrator, 'predict'):
        # å¯¹äºPlattç¼©æ”¾ï¼Œéœ€è¦2Dè¾“å…¥
        return calibrator.predict([[raw_prob]])[0]
    elif hasattr(calibrator, 'transform'):
        return calibrator.transform([raw_prob])[0]
    else:
        return raw_prob

def compute_brier_score(predictions: np.ndarray, outcomes: np.ndarray) -> float:
    """è®¡ç®—Brieråˆ†æ•°"""
    return np.mean((predictions - outcomes) ** 2)

def update_brier_score():
    """æ›´æ–°Brieråˆ†æ•°å†å²"""
    # ä»æœ€è¿‘äº¤æ˜“ä¸­è·å–é¢„æµ‹æ¦‚ç‡å’Œå®é™…ç»“æœ
    trades = st.session_state.trade_log[-100:]
    if len(trades) < 10:
        return
    df = pd.DataFrame(trades)
    # éœ€è¦è®°å½•æ¯ä¸ªäº¤æ˜“çš„é¢„æµ‹æ¦‚ç‡ï¼Œä½†å½“å‰æ—¥å¿—æ²¡æœ‰ï¼Œè¿™é‡Œç®€åŒ–ä½¿ç”¨ä¿¡å·æ¦‚ç‡ä½œä¸ºé¢„æµ‹
    # å®é™…åº”å­˜å‚¨é¢„æµ‹æ¦‚ç‡
    probs = np.random.rand(len(df))  # å ä½ï¼Œå®é™…åº”ä½¿ç”¨å­˜å‚¨çš„æ¦‚ç‡
    outcomes = (df['pnl'] > 0).astype(int).values
    brier = compute_brier_score(probs, outcomes)
    st.session_state.brier_scores.append(brier)

# ==================== è´å¶æ–¯å› å­æƒé‡æ›´æ–° ====================
def bayesian_update_factor_weights(ic_dict: Dict[str, List[float]]):
    global factor_weights
    prior_mean = 1.0
    prior_strength = CONFIG.bayesian_prior_strength
    for factor, ic_list in ic_dict.items():
        if len(ic_list) < 5:
            continue
        sample_mean = np.mean(ic_list)
        sample_std = np.std(ic_list)
        n = len(ic_list)
        posterior_mean = (prior_strength * prior_mean + n * sample_mean) / (prior_strength + n)
        factor_weights[factor] = max(0.1, posterior_mean)

def update_factor_ic_stats(ic_records: Dict[str, List[float]]):
    stats = {}
    for factor, ic_list in ic_records.items():
        if len(ic_list) > 5:
            mean_ic = np.mean(ic_list)
            std_ic = np.std(ic_list)
            ir = mean_ic / max(std_ic, 0.001)
            t_stat, p_value = ttest_1samp(ic_list, 0)
            stats[factor] = {'mean': mean_ic, 'std': std_ic, 'ir': ir, 'p_value': p_value}
            # è®°å½•ICå†å²
            if factor in st.session_state.ic_history:
                st.session_state.ic_history[factor].append(mean_ic)
    st.session_state.factor_ic_stats = stats

def calculate_ic(df: pd.DataFrame, factor_name: str) -> float:
    try:
        df_hash = pd.util.hash_pandas_object(df).sum()
    except:
        df_hash = id(df)
    key = (df_hash, factor_name)
    if key in _ic_cache:
        return _ic_cache[key]
    window = min(CONFIG.ic_window, len(df) - 6)
    if window < 20:
        return 0.0
    factor = df[factor_name].iloc[-window:-5]
    future = df['future_ret'].iloc[-window:-5]
    valid = factor.notna() & future.notna()
    if valid.sum() < 10:
        return 0.0
    ic = factor[valid].corr(future[valid])
    ic = 0.0 if pd.isna(ic) else ic
    _ic_cache[key] = ic
    return ic

_ic_cache = {}

def calculate_cov_matrix(symbols: List[str], data_dicts: Dict[str, Dict[str, pd.DataFrame]], window: int = 50) -> Optional[np.ndarray]:
    if len(symbols) < 2:
        return None
    hash_input = str(sorted(symbols)) + str(window)
    for sym in symbols:
        df = data_dicts[sym]['15m']['close'].iloc[-window:]
        hash_input += str(pd.util.hash_pandas_object(df).sum())
    cache_key = hashlib.md5(hash_input.encode()).hexdigest()
    if st.session_state.cov_matrix_cache.get('key') == cache_key:
        return st.session_state.cov_matrix_cache['matrix']
    returns_list = []
    for sym in symbols:
        df = data_dicts[sym]['15m']['close'].iloc[-window:]
        ret = df.pct_change().dropna().values
        if len(ret) < window // 2:
            return None
        returns_list.append(ret[-window:])
    returns_array = np.array(returns_list)
    if returns_array.shape[0] != len(symbols):
        return None
    cov = np.cov(returns_array)
    st.session_state.cov_matrix_cache = {'key': cache_key, 'matrix': cov}
    return cov

def risk_parity_weights(cov: np.ndarray) -> np.ndarray:
    n = cov.shape[0]
    vols = np.sqrt(np.diag(cov))
    inv_vol = 1.0 / vols
    w0 = inv_vol / np.sum(inv_vol)
    return w0

def black_litterman_weights(cov: np.ndarray, market_cap_weights: Optional[np.ndarray] = None, views: Optional[Dict] = None) -> np.ndarray:
    if market_cap_weights is None:
        market_cap_weights = np.ones(cov.shape[0]) / cov.shape[0]
    pi = CONFIG.black_litterman_tau * cov @ market_cap_weights
    return market_cap_weights

def allocate_with_risk_budget(symbols: List[str], cov: np.ndarray, balance: float, signals: Dict[str, Tuple]) -> Dict[str, float]:
    if CONFIG.risk_budget_method == 'risk_parity':
        weights = risk_parity_weights(cov)
    elif CONFIG.risk_budget_method == 'black_litterman':
        weights = black_litterman_weights(cov)
    else:
        weights = np.ones(len(symbols)) / len(symbols)
    allocations = {}
    for i, sym in enumerate(symbols):
        if sym in signals and signals[sym][0] != 0 and signals[sym][1] >= SignalStrength.WEAK.value:
            allocations[sym] = balance * weights[i]
        else:
            allocations[sym] = 0.0
    return allocations

def advanced_slippage_prediction(price: float, size: float, volume_20: float, volatility: float, imbalance: float) -> float:
    base_slippage = dynamic_slippage(price, size, volume_20, volatility, imbalance)
    market_impact = (size / max(volume_20, 1)) ** 0.5 * volatility * price * 0.3
    return base_slippage + market_impact

def dynamic_slippage(price: float, size: float, volume: float, volatility: float, imbalance: float = 0.0) -> float:
    base = price * CONFIG.slippage_base
    impact = CONFIG.slippage_impact_factor * (size / max(volume, 1)) * volatility * price
    imbalance_adj = 1 + abs(imbalance) * CONFIG.slippage_imbalance_factor
    return (base + impact) * imbalance_adj

def portfolio_var(weights: np.ndarray, cov: np.ndarray, confidence: float = 0.95, method: str = "HISTORICAL", historical_returns: Optional[np.ndarray] = None) -> float:
    if weights is None or cov is None or len(weights) == 0:
        return 0.0
    if method == "HISTORICAL" and historical_returns is not None and historical_returns.shape[1] > 20:
        port_rets = weights @ historical_returns
        var = np.percentile(port_rets, (1 - confidence) * 100)
        return abs(var)
    elif method == "EXTREME" and historical_returns is not None and historical_returns.shape[1] > 50:
        port_rets = weights @ historical_returns
        threshold = np.percentile(port_rets, 10)
        excess = port_rets[port_rets < threshold] - threshold
        if len(excess) < 10:
            return 0.0
        params = genpareto.fit(excess)
        var = threshold + genpareto.ppf(1 - confidence, *params)
        return abs(var)
    elif method == "VOLCONE":
        port_vol = np.sqrt(np.dot(weights.T, np.dot(cov, weights)))
        var = port_vol * norm.ppf(confidence)
        return abs(var)
    else:
        port_vol = np.sqrt(np.dot(weights.T, np.dot(cov, weights)))
        var = port_vol * norm.ppf(confidence)
        return abs(var)

def portfolio_cvar(weights: np.ndarray, historical_returns: np.ndarray, confidence: float = 0.95) -> float:
    if historical_returns is None or historical_returns.shape[1] < 20:
        return 0.0
    port_rets = weights @ historical_returns
    var = np.percentile(port_rets, (1 - confidence) * 100)
    cvar = port_rets[port_rets <= var].mean()
    return abs(cvar)

def get_dynamic_var_limit():
    base_limit = CONFIG.portfolio_risk_target * 100
    if st.session_state.get('aggressive_mode', False):
        base_limit = CONFIG.var_aggressive_threshold
    if is_night_time():
        base_limit *= CONFIG.night_risk_multiplier
    return base_limit

def is_night_time() -> bool:
    eastern = pytz.timezone('US/Eastern')
    now_eastern = datetime.now(pytz.utc).astimezone(eastern)
    hour = now_eastern.hour
    return CONFIG.night_start_hour <= hour < CONFIG.night_end_hour

def funding_rate_blocked(symbol: str, direction: int) -> bool:
    rate = st.session_state.funding_rates.get(symbol, 0.0)
    if abs(rate) > CONFIG.funding_rate_threshold / 100:
        if (rate > 0 and direction == -1) or (rate < 0 and direction == 1):
            log_execution(f"èµ„é‡‘è´¹ç‡é˜»æ­¢å¼€ä»“ {symbol} æ–¹å‘ {'å¤š' if direction==1 else 'ç©º'} è´¹ç‡ {rate*100:.4f}%")
            return True
    return False

# ==================== ä¿®å¤ç‰ˆ is_range_marketï¼ˆå¸¦ç©ºå€¼ä¿æŠ¤å’Œé˜²å¾¡ï¼‰====================
def is_range_market(df_dict: Dict[str, pd.DataFrame]) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºéœ‡è¡å¸‚åœºï¼Œå¢åŠ ç©ºå€¼ä¿æŠ¤å’Œå¼‚å¸¸å¤„ç†"""
    if '15m' not in df_dict:
        return False
    df = df_dict['15m']
    if df.empty:
        return False
    last = df.iloc[-1]
    try:
        if hasattr(last, 'get') and last.get('bb_width') is not None and not pd.isna(last.get('bb_width')):
            if last['bb_width'] < CONFIG.bb_width_threshold:
                return True
        if hasattr(last, 'get') and last.get('rsi') is not None and not pd.isna(last.get('rsi')):
            if CONFIG.rsi_range_low < last['rsi'] < CONFIG.rsi_range_high:
                return True
    except Exception as e:
        log_error(f"is_range_market åˆ¤æ–­å‡ºé”™: {e}")
        return False
    return False

def multi_timeframe_confirmation(df_dict: Dict[str, pd.DataFrame], direction: int) -> bool:
    count = 0
    for tf in CONFIG.confirm_timeframes:
        if tf not in df_dict:
            continue
        df = df_dict[tf]
        if df.empty:
            continue
        last = df.iloc[-1]
        if not pd.isna(last.get('ema20')):
            if (direction == 1 and last['close'] > last['ema20']) or (direction == -1 and last['close'] < last['ema20']):
                count += 1
    return count >= 2

def can_open_position(regime: MarketRegime) -> bool:
    return regime in CONFIG.regime_allow_trade

def dynamic_kelly_fraction() -> float:
    win_rate = st.session_state.performance_metrics.get('win_rate', 0.5)
    sharpe = st.session_state.performance_metrics.get('sharpe', 1.0)
    base = CONFIG.kelly_fraction
    discount = min(1.0, win_rate / 0.55) * min(1.0, sharpe / 1.5)
    return base * max(0.1, discount)

def update_factor_correlation(ic_records: Dict[str, List[float]]):
    global factor_corr_matrix
    if len(ic_records) < 2:
        return
    all_factors = list(factor_weights.keys())
    df_dict = {}
    for f in all_factors:
        if f in ic_records and ic_records[f]:
            df_dict[f] = pd.Series(ic_records[f])
        else:
            df_dict[f] = pd.Series([np.nan])
    ic_df = pd.DataFrame(df_dict)
    corr = ic_df.corr().fillna(0)
    factor_corr_matrix = corr.values

def apply_factor_correlation_penalty():
    global factor_weights
    if factor_corr_matrix is None:
        return
    factors = list(factor_weights.keys())
    n = len(factors)
    if factor_corr_matrix.shape[0] < n or factor_corr_matrix.shape[1] < n:
        return
    for i in range(n):
        for j in range(i+1, n):
            if factor_corr_matrix[i, j] > CONFIG.factor_corr_threshold:
                factor_weights[factors[i]] *= CONFIG.factor_corr_penalty
                factor_weights[factors[j]] *= CONFIG.factor_corr_penalty

def eliminate_poor_factors():
    global factor_weights
    for factor, stats in st.session_state.factor_ic_stats.items():
        if stats['p_value'] > CONFIG.factor_eliminate_pvalue and stats['mean'] < CONFIG.factor_eliminate_ic and len(ic_decay_records[factor]) > 30:
            factor_weights[factor] = CONFIG.factor_min_weight
            log_execution(f"å› å­æ·˜æ±°ï¼š{factor} æƒé‡é™è‡³{CONFIG.factor_min_weight}")

def generate_simulated_data(symbol: str, limit: int = 2000) -> Dict[str, pd.DataFrame]:
    seed = int(hashlib.md5(symbol.encode()).hexdigest()[:8], 16) % 2**32
    np.random.seed(seed)
    end = datetime.now()
    timestamps = pd.date_range(end=end, periods=limit, freq='15min')
    
    if 'BTC' in symbol:
        base = 40000
        volatility = CONFIG.sim_volatility * 0.6
        trend_factor = 0.1
    elif 'ETH' in symbol:
        base = 2100
        volatility = CONFIG.sim_volatility
        trend_factor = 0.15
    else:
        base = 100
        volatility = CONFIG.sim_volatility * 1.2
        trend_factor = 0.2
    
    t = np.linspace(0, 6*np.pi, limit)
    trend_direction = np.random.choice([-1, 1], p=[0.3, 0.7])
    trend = trend_direction * CONFIG.sim_trend_strength * np.linspace(0, 1, limit) * base * trend_factor
    cycle1 = 0.03 * base * np.sin(t * 1)
    cycle2 = 0.015 * base * np.sin(t * 3)
    cycle3 = 0.007 * base * np.sin(t * 7)
    random_step = np.random.randn(limit) * volatility * base
    random_walk = np.cumsum(random_step) * 0.15
    price_series = base + trend + cycle1 + cycle2 + cycle3 + random_walk
    price_series = np.maximum(price_series, base * 0.3)
    
    opens = price_series * (1 + np.random.randn(limit) * 0.0015)
    closes = price_series * (1 + np.random.randn(limit) * 0.0025)
    highs = np.maximum(opens, closes) + np.abs(np.random.randn(limit)) * volatility * price_series * 0.5
    lows = np.minimum(opens, closes) - np.abs(np.random.randn(limit)) * volatility * price_series * 0.5
    volume_base = np.random.randint(800, 8000, limit)
    volume_factor = 1 + 3 * np.abs(np.diff(price_series, prepend=price_series[0])) / price_series
    volumes = (volume_base * volume_factor).astype(int)
    
    df_15m = pd.DataFrame({
        'timestamp': timestamps,
        'open': opens,
        'high': highs,
        'low': lows,
        'close': closes,
        'volume': volumes
    })
    df_15m = add_indicators(df_15m)
    
    data_dict = {'15m': df_15m}
    for tf in ['1m', '5m', '1h', '4h', '1d']:
        resampled = df_15m.resample(tf, on='timestamp').agg({
            'open': 'first',
            'high': 'max',
            'low': 'min',
            'close': 'last',
            'volume': 'sum'
        }).dropna().reset_index()
        if len(resampled) >= 30:
            resampled = add_indicators(resampled)
            data_dict[tf] = resampled
    return data_dict

def add_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """æ·»åŠ æŠ€æœ¯æŒ‡æ ‡ï¼Œç¡®ä¿å¸ƒæ—å¸¦å®½åº¦å®‰å…¨è®¡ç®—"""
    df = df.copy()
    df['ema20'] = ta.trend.ema_indicator(df['close'], window=20)
    df['ema50'] = ta.trend.ema_indicator(df['close'], window=50)
    df['ema200'] = ta.trend.ema_indicator(df['close'], window=200)
    if len(df) >= 14:
        df['rsi'] = ta.momentum.rsi(df['close'], window=14)
        atr = ta.volatility.average_true_range(df['high'], df['low'], df['close'], window=14)
        df['atr'] = atr
        df['atr_ma'] = atr.rolling(20).mean()
    else:
        df['rsi'] = np.nan
        df['atr'] = np.nan
        df['atr_ma'] = np.nan
    if len(df) >= 26:
        macd = ta.trend.MACD(df['close'])
        df['macd'] = macd.macd()
        df['macd_signal'] = macd.macd_signal()
        df['macd_diff'] = df['macd'] - df['macd_signal']
    else:
        df['macd'] = np.nan
        df['macd_signal'] = np.nan
        df['macd_diff'] = np.nan
    if len(df) >= 14:
        try:
            df['adx'] = ta.trend.adx(df['high'], df['low'], df['close'], window=14)
        except:
            df['adx'] = np.nan
    else:
        df['adx'] = np.nan
    if len(df) >= CONFIG.bb_window:
        bb = ta.volatility.BollingerBands(df['close'], window=CONFIG.bb_window, window_dev=2)
        df['bb_upper'] = bb.bollinger_hband()
        df['bb_lower'] = bb.bollinger_lband()
        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['close']
        df['bb_factor'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
    else:
        df['bb_upper'] = np.nan
        df['bb_lower'] = np.nan
        df['bb_width'] = np.nan
        df['bb_factor'] = np.nan
    df['volume_sma'] = df['volume'].rolling(20).mean()
    df['volume_ratio'] = df['volume'] / df['volume_sma']
    df['trend_factor'] = (df['close'] - df['ema20']) / df['close']
    if len(df) >= 6:
        df['future_ret'] = df['close'].pct_change(5).shift(-5)
    else:
        df['future_ret'] = np.nan
    return df

@st.cache_data(ttl=3600, show_spinner=False)
def fetch_fear_greed() -> int:
    try:
        r = requests.get("https://api.alternative.me/fng/?limit=1", timeout=5)
        return int(r.json()['data'][0]['value'])
    except Exception:
        return 50

@st.cache_resource
def get_fetcher() -> 'AggregatedDataFetcher':
    return AggregatedDataFetcher()

class AggregatedDataFetcher:
    def __init__(self):
        self.exchanges: Dict[str, ccxt.Exchange] = {}
        self.executor = ThreadPoolExecutor(max_workers=5)
        for name in CONFIG.data_sources:
            try:
                cls = getattr(ccxt, name)
                ex = cls({'enableRateLimit': True, 'timeout': 30000, 'options': {'defaultType': 'future'}})
                self.exchanges[name] = ex
            except Exception:
                pass

    @safe_request(max_retries=3, default=None)
    def _fetch_kline_single(self, ex: ccxt.Exchange, symbol: str, timeframe: str, limit: int) -> Optional[pd.DataFrame]:
        ohlcv = ex.fetch_ohlcv(symbol, timeframe=timeframe, limit=limit)
        if ohlcv and len(ohlcv) >= 50:
            df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df = df.astype({col: float for col in ['open','high','low','close','volume']})
            return df
        return None

    def _fetch_kline_parallel(self, symbol: str, timeframe: str, limit: int) -> Optional[pd.DataFrame]:
        futures = []
        for name in ["binance"] + [n for n in CONFIG.data_sources if n != "binance"]:
            if name in self.exchanges:
                ex = self.exchanges[name]
                futures.append(self.executor.submit(self._fetch_kline_single, ex, symbol, timeframe, limit))
        for future in as_completed(futures):
            result = future.result(timeout=10)
            if result is not None:
                for f in futures:
                    f.cancel()
                return result
        return None

    def fetch_all_timeframes(self, symbol: str) -> Dict[str, pd.DataFrame]:
        data_dict = {}
        all_tfs = list(set(CONFIG.timeframes + CONFIG.confirm_timeframes))
        for tf in all_tfs:
            df = self._fetch_kline_parallel(symbol, tf, CONFIG.fetch_limit)
            if df is not None and len(df) >= 50:
                df = add_indicators(df)
                data_dict[tf] = df
        return data_dict

    @safe_request(max_retries=3, default=0.0)
    def fetch_funding_rate(self, symbol: str) -> float:
        rates = []
        for name in ["binance"] + [n for n in CONFIG.data_sources if n != "binance"]:
            if name in self.exchanges:
                try:
                    rates.append(self.exchanges[name].fetch_funding_rate(symbol)['fundingRate'])
                except Exception:
                    continue
        return float(np.mean(rates)) if rates else 0.0

    @safe_request(max_retries=3, default=0.0)
    def fetch_orderbook_imbalance(self, symbol: str, depth: int = 10) -> float:
        for name in ["binance"] + [n for n in CONFIG.data_sources if n != "binance"]:
            if name in self.exchanges:
                ob = self.exchanges[name].fetch_order_book(symbol, limit=depth)
                bid_vol = sum(b[1] for b in ob['bids'])
                ask_vol = sum(a[1] for a in ob['asks'])
                total = bid_vol + ask_vol
                return (bid_vol - ask_vol) / total if total > 0 else 0.0
        return 0.0

    def get_symbol_data(self, symbol: str) -> Optional[Dict[str, Any]]:
        if st.session_state.use_simulated_data:
            sim_data = generate_simulated_data(symbol)
            return {
                "data_dict": sim_data,
                "current_price": sim_data['15m']['close'].iloc[-1],
                "fear_greed": 50,
                "funding_rate": 0.0,
                "orderbook_imbalance": 0.0,
            }
        data_dict = self.fetch_all_timeframes(symbol)
        if '15m' not in data_dict or data_dict['15m'].empty:
            log_error(f"è·å– {symbol} æ•°æ®å¤±è´¥ï¼Œè‡ªåŠ¨åˆ‡æ¢æ¨¡æ‹Ÿæ¨¡å¼")
            st.session_state.use_simulated_data = True
            sim_data = generate_simulated_data(symbol)
            return {
                "data_dict": sim_data,
                "current_price": sim_data['15m']['close'].iloc[-1],
                "fear_greed": 50,
                "funding_rate": 0.0,
                "orderbook_imbalance": 0.0,
            }
        current_price = float(data_dict['15m']['close'].iloc[-1])
        funding = self.fetch_funding_rate(symbol)
        st.session_state.funding_rates[symbol] = funding
        # æ›´æ–°å¦ç±»æ•°æ®
        update_alternative_data(symbol)
        return {
            "data_dict": data_dict,
            "current_price": current_price,
            "fear_greed": fetch_fear_greed(),
            "funding_rate": funding,
            "orderbook_imbalance": self.fetch_orderbook_imbalance(symbol),
        }

class SignalEngine:
    def __init__(self):
        pass

    def detect_market_regime(self, df_dict: Dict[str, pd.DataFrame], symbol: str) -> MarketRegime:
        return detect_market_regime_advanced(df_dict, symbol)

    def calc_signal(self, df_dict: Dict[str, pd.DataFrame], symbol: str) -> Tuple[int, float]:
        global factor_weights, ic_decay_records
        total_score = 0
        total_weight = 0
        tf_votes = []
        regime = st.session_state.market_regime
        ic_dict = {}

        try:
            range_penalty = 0.5 if is_range_market(df_dict) else 1.0
        except Exception as e:
            log_error(f"is_range_market è°ƒç”¨å¼‚å¸¸: {e}")
            range_penalty = 1.0

        for tf, df in df_dict.items():
            if df.empty or len(df) < 2:
                continue
            last = df.iloc[-1]
            weight = CONFIG.timeframe_weights.get(tf, 1) * range_penalty
            if regime == MarketRegime.TREND:
                if tf in ['4h', '1d']:
                    weight *= 1.5
            elif regime == MarketRegime.RANGE:
                if tf in ['15m', '1h']:
                    weight *= 1.3
            if pd.isna(last.get('ema20', np.nan)):
                continue

            factor_scores = {}
            if last['close'] > last['ema20']:
                factor_scores['trend'] = 1 * factor_weights['trend']
            elif last['close'] < last['ema20']:
                factor_scores['trend'] = -1 * factor_weights['trend']
            else:
                factor_scores['trend'] = 0

            if last['rsi'] > 70:
                factor_scores['rsi'] = -0.7 * factor_weights['rsi']
            elif last['rsi'] < 30:
                factor_scores['rsi'] = 0.7 * factor_weights['rsi']
            else:
                factor_scores['rsi'] = 0

            if last['macd_diff'] > 0:
                factor_scores['macd'] = 0.8 * factor_weights['macd']
            elif last['macd_diff'] < 0:
                factor_scores['macd'] = -0.8 * factor_weights['macd']
            else:
                factor_scores['macd'] = 0

            if not pd.isna(last.get('bb_upper')):
                if last['close'] > last['bb_upper']:
                    factor_scores['bb'] = -0.5 * factor_weights['bb']
                elif last['close'] < last['bb_lower']:
                    factor_scores['bb'] = 0.5 * factor_weights['bb']
                else:
                    factor_scores['bb'] = 0
            else:
                factor_scores['bb'] = 0

            if not pd.isna(last.get('volume_ratio')):
                factor_scores['volume'] = (1.2 if last['volume_ratio'] > 1.5 else 0) * factor_weights['volume']
            else:
                factor_scores['volume'] = 0

            adx = last.get('adx', 25)
            if pd.isna(adx):
                factor_scores['adx'] = 0
            else:
                factor_scores['adx'] = (0.3 if adx > 30 else -0.2 if adx < 20 else 0) * factor_weights['adx']

            if CONFIG.use_ml_factor:
                ml_score = get_ml_factor(symbol, df_dict)
                factor_scores['ml'] = ml_score * factor_weights['ml']

            for fname in factor_scores.keys():
                col = factor_to_col.get(fname)
                if col and col in df.columns:
                    ic = calculate_ic(df, col)
                    if not np.isnan(ic):
                        if fname not in ic_dict:
                            ic_dict[fname] = []
                        ic_dict[fname].append(ic)
                        ic_decay_records[fname].append(ic)

            tf_score = sum(factor_scores.values()) * weight
            total_score += tf_score
            total_weight += weight
            if tf_score > 0:
                tf_votes.append(1)
            elif tf_score < 0:
                tf_votes.append(-1)

        bayesian_update_factor_weights(ic_dict)
        update_factor_correlation(ic_dict)
        apply_factor_correlation_penalty()
        update_factor_ic_stats(ic_dict)
        eliminate_poor_factors()

        if total_weight == 0:
            return 0, 0.0
        max_possible = sum(CONFIG.timeframe_weights.values()) * 3.5
        prob_raw = min(1.0, abs(total_score) / max_possible) if max_possible > 0 else 0.5
        prob = 0.5 + 0.45 * prob_raw

        # æ¦‚ç‡æ ¡å‡†
        if CONFIG.use_prob_calibration and symbol in ml_calibrators:
            prob = apply_calibration(symbol, prob)

        direction_candidate = 1 if total_score > 0 else -1 if total_score < 0 else 0
        if direction_candidate != 0 and not multi_timeframe_confirmation(df_dict, direction_candidate):
            prob *= 0.5

        if prob < SignalStrength.WEAK.value:
            return 0, prob

        if prob >= SignalStrength.WEAK.value:
            direction = direction_candidate
        else:
            if tf_votes:
                direction = 1 if sum(tf_votes) > 0 else -1 if sum(tf_votes) < 0 else 0
            else:
                direction = 0
        if direction == 0:
            prob = 0.0
        return direction, prob

# ==================== ç‹¬ç«‹é£é™©å¼•æ“ ====================
class RiskEngine:
    def __init__(self):
        pass

    def get_portfolio_var(self, weights, cov, confidence=0.95):
        """è®¡ç®—ç»„åˆVaR"""
        if weights is None or cov is None or len(weights) == 0:
            return 0.0
        port_vol = np.sqrt(np.dot(weights.T, np.dot(cov, weights)))
        return port_vol * norm.ppf(confidence)

    def get_current_drawdown(self):
        """è·å–å½“å‰å›æ’¤"""
        current_dd, _ = calculate_drawdown()
        return current_dd

    def scaling_factor(self):
        """åŠ¨æ€ä»“ä½ç¼©æ”¾å› å­"""
        factor = 1.0
        # å›æ’¤ç¼©æ”¾
        dd = self.get_current_drawdown()
        if dd > 5.0:
            factor *= (1 - (dd - 5.0) / CONFIG.drawdown_scaling_factor)
        # VaRç¼©æ”¾
        if st.session_state.cov_matrix is not None and len(st.session_state.positions) > 0:
            total_value = st.session_state.account_balance
            weights = []
            for sym in st.session_state.current_symbols:
                if sym in st.session_state.positions:
                    pos = st.session_state.positions[sym]
                    value = pos.size * st.session_state.symbol_current_prices.get(sym, 1)
                    weights.append(value / total_value)
                else:
                    weights.append(0.0)
            weights = np.array(weights)
            if np.sum(weights) > 0:
                weights = weights / np.sum(weights)
                var = self.get_portfolio_var(weights, st.session_state.cov_matrix)
                target_var = CONFIG.portfolio_risk_target
                if var > target_var:
                    factor *= target_var / var
        # ç¡®ä¿å› å­åœ¨åˆç†èŒƒå›´å†…
        return max(0.1, min(1.0, factor))

    def check_circuit_breaker(self):
        """ç†”æ–­æ£€æŸ¥"""
        # æ—¥äºæŸè¶…é™
        daily_loss = -st.session_state.daily_pnl
        if daily_loss > st.session_state.account_balance * CONFIG.daily_loss_limit:
            return True, f"æ—¥äºæŸ{daily_loss:.2f}è¶…é™"
        # è¿ç»­äºæŸè¶…é™
        if st.session_state.consecutive_losses >= CONFIG.max_consecutive_losses:
            return True, f"è¿ç»­äºæŸ{st.session_state.consecutive_losses}æ¬¡"
        # ATRç†”æ–­
        # ç®€å•å®ç°ï¼šæ£€æŸ¥ä»»æ„æŒä»“çš„ATRç™¾åˆ†æ¯”
        for sym, pos in st.session_state.positions.items():
            if sym in st.session_state.multi_df:
                atr = st.session_state.multi_df[sym]['15m']['atr'].iloc[-1]
                price = st.session_state.symbol_current_prices.get(sym, 1)
                atr_pct = atr / price * 100
                if atr_pct > CONFIG.circuit_breaker_atr:
                    return True, f"{sym} ATRç™¾åˆ†æ¯”{atr_pct:.2f}%è¶…é™"
        return False, ""

    def stress_test(self, scenario):
        """å‹åŠ›æµ‹è¯•ï¼šåº”ç”¨åœºæ™¯å¹¶è®¡ç®—ç»„åˆæŸç›Š"""
        # ç®€åŒ–ï¼šè®¡ç®—å„å“ç§ä»·æ ¼å˜åŠ¨åçš„ç»„åˆæŸç›Š
        total_pnl = 0
        for sym, pos in st.session_state.positions.items():
            if sym in scenario:
                change = scenario[sym]  # ä¾‹å¦‚ -0.1 è¡¨ç¤ºè·Œ10%
                new_price = st.session_state.symbol_current_prices.get(sym, 1) * (1 + change)
                pnl = pos.pnl(new_price) - pos.pnl(st.session_state.symbol_current_prices.get(sym, 1))
                total_pnl += pnl
        return total_pnl

    def run_all_stress_tests(self):
        """è¿è¡Œæ‰€æœ‰é¢„è®¾å‹åŠ›åœºæ™¯"""
        results = {}
        for name, scenario in CONFIG.stress_scenarios.items():
            if 'volatility_multiplier' in scenario:
                # æ³¢åŠ¨ç‡åœºæ™¯ï¼Œæš‚æ—¶è·³è¿‡
                continue
            pnl = self.stress_test(scenario)
            results[name] = pnl
        return results

    def extreme_risk_detection(self):
        """æ£€æµ‹æç«¯é£é™©ï¼ˆRegime Shiftï¼‰"""
        # æ³¢åŠ¨ç‡è·³è·ƒ
        if len(st.session_state.daily_returns) > 20:
            recent_vol = np.std(list(st.session_state.daily_returns)[-10:]) * np.sqrt(252)
            older_vol = np.std(list(st.session_state.daily_returns)[-20:-10]) * np.sqrt(252)
            if older_vol > 0 and recent_vol / older_vol > 2.0:
                return True, "æ³¢åŠ¨ç‡è·³è·ƒ"
        # å› å­ICå´©å¡Œ
        for factor, stats in st.session_state.factor_ic_stats.items():
            if factor in st.session_state.ic_history and len(st.session_state.ic_history[factor]) > 10:
                recent_ic = np.mean(list(st.session_state.ic_history[factor])[-5:])
                older_ic = np.mean(list(st.session_state.ic_history[factor])[-10:-5])
                if older_ic > 0 and recent_ic < older_ic * 0.5:
                    return True, f"{factor} ICå´©å¡Œ"
        return False, ""

risk_engine = RiskEngine()

# ==================== æ‰§è¡Œå±‚ä¼˜åŒ– ====================
def check_liquidity(symbol: str, size: float) -> bool:
    """æ£€æŸ¥è®¢å•å¤§å°æ˜¯å¦è¶…è¿‡ç›˜å£æ·±åº¦é™åˆ¶"""
    if not st.session_state.exchange:
        return True
    try:
        ob = st.session_state.exchange.fetch_order_book(symbol, limit=10)
        # ç®€å•å–å‰10æ¡£çš„æ€»é‡
        total_bid_vol = sum(b[1] for b in ob['bids'])
        total_ask_vol = sum(a[1] for a in ob['asks'])
        depth = max(total_bid_vol, total_ask_vol)
        if size > depth * CONFIG.max_order_to_depth_ratio:
            log_execution(f"æµåŠ¨æ€§ä¸è¶³ï¼šè®¢å•å¤§å°{size:.4f}è¶…è¿‡ç›˜å£æ·±åº¦{depth:.4f}çš„{CONFIG.max_order_to_depth_ratio*100:.1f}%")
            return False
        return True
    except:
        return True

def simulate_latency():
    """æ¨¡æ‹Ÿå»¶è¿Ÿ"""
    time.sleep(CONFIG.latency_sim_ms / 1000.0)

# ==================== ç³»ç»Ÿè‡ªæ£€æ¨¡å— ====================
def self_check():
    """æ¯æ—¥ç³»ç»Ÿè‡ªæ£€"""
    global last_self_check_time
    now = time.time()
    if now - last_self_check_time < CONFIG.self_check_interval:
        return
    last_self_check_time = now

    messages = []
    healthy = True

    # æ•°æ®ç¼ºå¤±æ£€æŸ¥
    for sym in st.session_state.current_symbols:
        if sym not in st.session_state.multi_df:
            messages.append(f"æ•°æ®ç¼ºå¤±ï¼š{sym}")
            healthy = False

    # å› å­å¼‚å¸¸ï¼ˆæƒé‡æç«¯ï¼‰
    for factor, w in factor_weights.items():
        if w < 0.01 or w > 10:
            messages.append(f"å› å­æƒé‡å¼‚å¸¸ï¼š{factor}={w:.4f}")
            healthy = False

    # ICéª¤é™
    for factor, stats in st.session_state.factor_ic_stats.items():
        if factor in st.session_state.ic_history and len(st.session_state.ic_history[factor]) > 10:
            recent_ic = np.mean(list(st.session_state.ic_history[factor])[-5:])
            older_ic = np.mean(list(st.session_state.ic_history[factor])[-10:-5])
            if older_ic > 0 and recent_ic < older_ic * CONFIG.anomaly_threshold_ic_drop:
                messages.append(f"{factor} ICéª¤é™ï¼š{recent_ic:.4f} vs {older_ic:.4f}")
                healthy = False

    # é£é™©ç»Ÿè®¡å¼‚å¸¸ï¼ˆVaRè¿‡å¤§ï¼‰
    if st.session_state.cov_matrix is not None and len(st.session_state.positions) > 0:
        total_value = st.session_state.account_balance
        weights = []
        for sym in st.session_state.current_symbols:
            if sym in st.session_state.positions:
                pos = st.session_state.positions[sym]
                value = pos.size * st.session_state.symbol_current_prices.get(sym, 1)
                weights.append(value / total_value)
            else:
                weights.append(0.0)
        weights = np.array(weights)
        if np.sum(weights) > 0:
            weights = weights / np.sum(weights)
            var = risk_engine.get_portfolio_var(weights, st.session_state.cov_matrix)
            if var > 0.05:  # 5% VaRé˜ˆå€¼
                messages.append(f"ç»„åˆVaRè¿‡é«˜ï¼š{var*100:.2f}%")
                healthy = False

    st.session_state.self_check_status = {"healthy": healthy, "messages": messages}
    if not healthy:
        log_error(f"è‡ªæ£€å¼‚å¸¸ï¼š{messages}")
        send_telegram(f"âš ï¸ ç³»ç»Ÿè‡ªæ£€å¼‚å¸¸ï¼š{messages}", msg_type="risk")
    else:
        log_execution("ç³»ç»Ÿè‡ªæ£€é€šè¿‡")

# ==================== å…¶ä»–åŸæœ‰å‡½æ•°ä¿æŒä¸å˜ï¼ŒåŒ…æ‹¬RiskManagerã€Positionã€execute_orderç­‰ï¼ˆç¯‡å¹…é™åˆ¶ï¼Œæ­¤å¤„ç•¥å»ï¼Œä½†åœ¨æœ€ç»ˆä»£ç ä¸­éœ€å®Œæ•´ä¿ç•™ï¼‰====================

# ç”±äºç¯‡å¹…é™åˆ¶ï¼Œåœ¨æ­¤çœç•¥äº†RiskManagerã€Positionã€execute_orderã€close_positionã€fix_data_consistencyã€generate_equity_chartã€run_backtestã€UIRendererç­‰å‡½æ•°çš„é‡å¤å®šä¹‰ï¼Œä½†åœ¨æœ€ç»ˆå®Œæ•´ä»£ç ä¸­å¿…é¡»åŒ…å«å®ƒä»¬ï¼ˆåŸºäºè¶…ç¥ç‰ˆ49.0çš„å®Œæ•´å†…å®¹ï¼Œå¹¶æ•´åˆä¸Šè¿°æ–°å¢æ¨¡å—ï¼‰ã€‚
# å®é™…æä¾›æ—¶ï¼Œåº”å°†ä»¥ä¸Šæ‰€æœ‰æ–°ä»£ç ä¸åŸæœ‰è¶…ç¥ç‰ˆ49.0çš„å®Œæ•´ä»£ç åˆå¹¶ï¼Œå¹¶é€‚å½“è°ƒæ•´UIRendererä»¥æ˜¾ç¤ºæ–°å¢çš„ç›‘æ§é¢æ¿ã€‚

# ==================== ä¸»ç¨‹åºå…¥å£ï¼ˆç®€åŒ–ï¼‰====================
def main():
    st.set_page_config(page_title="ç»ˆæé‡åŒ–ç»ˆç«¯ Â· å‡†æœºæ„ç‰ˆ 50.0", layout="wide")
    st.markdown("<style>.stApp { background: #0B0E14; color: white; }</style>", unsafe_allow_html=True)
    st.title("ğŸš€ ç»ˆæé‡åŒ–ç»ˆç«¯ Â· å‡†æœºæ„ç‰ˆ 50.0")
    st.caption("å®‡å®™ä¸»å®° | æ°¸æ’æ— æ•Œ | å®Œç¾æ— ç‘• | æ°¸ä¸è´¥åŒ— Â· ç‹¬ç«‹é£é™©å¼•æ“ Â· æ¦‚ç‡æ ¡å‡† Â· Walk-Forward Â· è’™ç‰¹å¡æ´› Â· è‡ªæ£€ç³»ç»Ÿ")

    init_session_state()
    check_and_fix_anomalies()
    self_check()  # æ¯æ—¥è‡ªæ£€

    # è¿™é‡Œéœ€è¦å®ä¾‹åŒ– UIRenderer å¹¶è°ƒç”¨ï¼Œä½†å› ç¯‡å¹…çœç•¥ï¼Œå®é™…ä»£ç ä¸­å¿…é¡»å®Œæ•´ä¿ç•™UIRendererç±»ã€‚
    # ä¸ºäº†æä¾›å¯ç›´æ¥è¿è¡Œçš„ä»£ç ï¼Œéœ€è¦å°†è¶…ç¥ç‰ˆ49.0ä¸­çš„UIRendereråŠå…¶æ‰€æœ‰ä¾èµ–å®Œæ•´å¤åˆ¶è¿‡æ¥ï¼Œå¹¶æ·»åŠ æ–°é€‰é¡¹å¡å±•ç¤ºé£é™©å¼•æ“ç»“æœã€å‹åŠ›æµ‹è¯•ã€è‡ªæ£€æŠ¥å‘Šç­‰ã€‚

    # ä»¥ä¸‹ä¸ºå ä½ï¼Œå®é™…è¿è¡Œæ—¶éœ€æ›¿æ¢ä¸ºå®Œæ•´UIRendererè°ƒç”¨
    st.info("å®Œæ•´ä»£ç å·²é›†æˆï¼Œè¯·ç¡®ä¿æ‰€æœ‰å‡½æ•°å®šä¹‰å®Œæ•´ã€‚")

    st_autorefresh(interval=CONFIG.auto_refresh_ms, key="auto_refresh")

if __name__ == "__main__":
    main()
